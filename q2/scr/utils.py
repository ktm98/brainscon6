import torch
import random
import os
import numpy as np
import math
import time
from collections import OrderedDict
from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve
import cv2

import torch.distributed as dist
import warnings

def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True  # False for Faster training
    torch.backends.cudnn.banchmark = False  # True for faster training

def get_score(y_true, y_pred):
    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)
    f1_scores = 2 * precision * recall / (precision + recall)
    max_idx = np.argmax(f1_scores)
    max_f1_score = f1_scores[max_idx]
    thresh = thresholds[max_idx]

    if np.isnan(max_f1_score):

        max_f1_score = f1_score(y_true, y_pred >= 0.5)
        thresh = 0.5
    return max_f1_score, thresh

class AverageMeter(object):
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count


def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)


def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))



def fix_model_state_dict(state_dict):
    new_state_dict = OrderedDict()
    for k, v in state_dict.items():
        name = k
        if name.startswith('backbone.'): # for dino teacher 
            name = name[9:]
        if name.startswith('module.head') or name.startswith('head'):
            continue
        if name.startswith('module.'):
            name = name[7:]  # remove 'module.' of dataparallel
            if name.startswith('backbone.'):  # for dino
                name = name[9:]
        if name.startswith('backbone.'): # for dino teacher 
            name = name[9:]
        new_state_dict[name] = v
        if name.startswith('module.head') or name.startswith('head'):
            pass
    return new_state_dict


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)



def setup(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355' # 適当な数字で設定すればいいらしいがよくわかっていない

    # initialize the process group
    dist.init_process_group("nccl", rank=rank, world_size=world_size)


def multi_getattr(obj, attr, default = None):
    """
    Get a named attribute from an object; multi_getattr(x, 'a.b.c.d') is
    equivalent to x.a.b.c.d. When a default argument is given, it is
    returned when any attribute in the chain doesn't exist; without
    it, an exception is raised when a missing attribute is encountered.
    ref: https://code.activestate.com/recipes/577346-getattr-with-arbitrary-depth/

    """
    attributes = attr.split(".")
    for i in attributes:
        try:
            obj = getattr(obj, i)
        except AttributeError:
            if default:
                return default
            else:
                raise
    return obj



def view_as_blocks_tensor(x, size):
    channels = x.shape[1]
    patches = torch.nn.Unfold(kernel_size=size, stride=size, padding=(0,0), dilation=(1,1))(x)
    patches = patches.permute([0, 2, 1]).reshape(-1, channels, *size)
    return patches


def split_patches(x, size, stride=None, padding=(0, 0)):
    channels = x.shape[1]
    if stride is None:
        stride = size
    
    patches = torch.nn.Unfold(kernel_size=size, stride=stride, padding=padding, dilation=(1,1))(x)
    patches = patches.permute([0, 2, 1]).reshape(-1, channels, *size)
    return patches

# ref: https://note.nkmk.me/python-opencv-hconcat-vconcat-np-tile/
def concat_tile(im_list_2d):
    return cv2.vconcat([cv2.hconcat(im_list_h) for im_list_h in im_list_2d])


